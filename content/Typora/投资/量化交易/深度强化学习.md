# SCI-FI

他应该跟你介绍过我在做一个实验性的策略，用的是强化学习里的PPO算法。选择强化学习因为我觉得它是机器学习里最偏向于智能的一个方向，跟人思考和决策的方式非常相似。这个算法是OPENAI用来训练DOTA2的bot的方法。我选这个方法主要是因为它在优化层面做得很好，而且大概半年前应该算是DRL里面最优的通用算法。



简单地说，我们先设计几个离散的动作（例如不操作，市价买1%，市价卖1%，限价买1%，限价卖1%），然后每一轮接收到市场的数据（state），我们就根据这个state算这5个action的概率分布，然后我们从这个概率分布里面抽样抽出一个action出来执行。过一段时间我们记录这个action对应的reward , 如果赚了，这个action在相似state情况下的概率就会变大，如果亏了，概率就会变小。



瓶颈：

现在遇到的瓶颈就是深度强化学习其实是比较暴力的算法，需要大量的数据，他们做游戏的完全可以靠模拟生成大量的对战数据，但是金融市场的数据在某一个时间点只有一份数据，所以是不是要想办法构造一批模拟数据，怎么模拟，都是比较大的问题。

第二个就是这个策略怎么启动，现在的初始条件是五个操作都是等概率的，这个在现实世界中肯定是很蠢的，其实做交易的比例应该非常小，但是这个比例我是人为地给他调小，好像有不是特别好。

