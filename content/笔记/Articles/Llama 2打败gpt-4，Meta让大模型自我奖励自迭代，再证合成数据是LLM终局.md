# Llama 2打败gpt-4，Meta让大模型自我奖励自迭代，再证合成数据是LLM终局

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/media/uploaded_book_covers/profile_101759/6c1a8180a4d350c32c11736e620c4125.jpg)

## Metadata
- Author: [[Tiếng Việt]]
- Full Title: Llama 2打败gpt-4，Meta让大模型自我奖励自迭代，再证合成数据是LLM终局
- Category: #articles
- Summary: AI训AI必将成为一大趋势。Meta和NYU团队提出了一种让大模型「自我奖励」的方法，通过迭代训练，让Llama2模型击败了GPT-4 0613、Claude 2、Gemini Pro等领先模型。这种方法允许模型生成训练数据，并评估这些数据的质量，然后用这些数据来自己训练自己。研究人员从Llama 2-70B预训练模型开始迭代训练，结果显示模型的遵循指令能力和打分能力都有显著提升，最终在AlpacaEval 2.0基准测试中击败了其他模型。这项工作在推动AI自我迭代大模型的前沿方面取得了重要进展。
- URL: https://followin.io/zh-Hans/feed/7666587

## Highlights
- 正如论文题目所言——「自我奖励语言模型」，模型生成训练数据，并评估这些数据的质量，然后用这些数据来自己训练自己。
  简单来说，最新方法可以让LLM在迭代训练过程中不断自我改进。 ([View Highlight](https://read.readwise.io/read/01hmqp77gnk4w3a4wajfktrp38))
